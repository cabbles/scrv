{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4961e3ed-11b9-4542-9df4-1dc90dfc4bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.4.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m160.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, hf-xet, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2025.7.0 hf-xet-1.1.5 huggingface-hub-0.33.4 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.2 tqdm-4.67.1 transformers-4.53.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.24.1)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "Successfully installed scipy-1.15.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.33.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.7.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m724.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-1.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#holay molay :0 ヾ(＠⌒ー⌒＠)ノ  ╚(•⌂•)╝   (≧∀≦)ゞ\n",
    "#finish:\n",
    "#min p add ✓\n",
    "#fix sample nexttoken ✓\n",
    "#logtoku detection\n",
    "###adapt get_eu for per-token basis ✓\n",
    "###implement R_token = -AU_token * EU_token ✓\n",
    "###logtoku fucntion(create sliding window evaluating r_response = (1/K) Σ R(aₜ) )\n",
    "#verify generation loop works ✓\n",
    "#verify hallucination detection works (⊙_⊙;)\n",
    "!pip install transformers\n",
    "!pip install scipy\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b41d7fc-fd97-4751-b292-03a644f1cfe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m     11\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     13\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import softmax, digamma\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "\n",
    "#chatgpt said this would get rid of variation when greedy decoding\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c77ab3f-d66c-4bd0-8da6-68278e6d899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topk, \n",
    "#get_uncertainty_metric (renamed from get_eu), \n",
    "#get_one_pass_metric for logtoku pasted from their codebase\n",
    "\n",
    "def topk(arr, k):\n",
    "    indices = np.argpartition(arr, -k)[-k:]\n",
    "    values = arr[indices]\n",
    "    return values, indices\n",
    "    \n",
    "def get_uncertainty_metric(logits, mode=\"eu\", k=25):\n",
    "    if mode == \"eu\":\n",
    "        if k is None:\n",
    "            raise ValueError(\"k must be provided for 'eu' mode.\")\n",
    "\n",
    "        def eu(logits):\n",
    "            top_k = k\n",
    "            if len(logits) < top_k:\n",
    "                raise ValueError(f\"Logits array length ({len(logits)}) is less than top_k (= {k}). \\nLogits shape: {logits.shape}, first 10 logits: {logits.squeeze(0).cpu().numpy()[:10]}\")\n",
    "            top_values, _ = topk(logits, top_k)\n",
    "            mean_scores = top_k / (np.sum(np.maximum(0, top_values)) + top_k)\n",
    "            return mean_scores\n",
    "        return eu\n",
    "\n",
    "    \n",
    "    elif mode == \"au\":\n",
    "        def cal_au(logits):\n",
    "            top_k = k\n",
    "            if len(logits) < top_k:\n",
    "                raise ValueError(\"Logits array length is less than top_k.\")\n",
    "            top_values = np.partition(logits, -top_k)[-top_k:]\n",
    "            alpha = np.array([top_values])\n",
    "            alpha_0 = alpha.sum(axis=1, keepdims=True)\n",
    "            psi_alpha_k_plus_1 = digamma(alpha + 1)\n",
    "            psi_alpha_0_plus_1 = digamma(alpha_0 + 1)\n",
    "            result = - (alpha / alpha_0) * (psi_alpha_k_plus_1 - psi_alpha_0_plus_1)\n",
    "            print(f\"k = {k}\")\n",
    "            return result.sum(axis=1)[0]\n",
    "        return cal_au\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "def get_token_reliability(logits, k=25):\n",
    "    au_fn = get_uncertainty_metric(logits, mode= \"au\", k=k)\n",
    "    eu_fn = get_uncertainty_metric(logits, mode= \"eu\", k=k)   \n",
    "    au = au_fn(logits)\n",
    "    eu = eu_fn(logits)\n",
    "    print(f\"au: {au}, eu: {eu}, rel.: {-au * eu}\")\n",
    "    return -au * eu\n",
    "\n",
    "\n",
    "#unused so far?\n",
    "def get_one_pass_metric(logits, clean_generated_tokens_length, metrics, get_eu, topk):\n",
    "    \"\"\"\n",
    "    Process logits, calculate various metrics, and extract topk information from the logits.\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "        clean_generated_tokens_length (int): Length of the generated tokens.\n",
    "        metrics (list): List of metrics and their parameters.\n",
    "        get_eu (function): Function to retrieve metric values.\n",
    "        topk (function): Function to extract topk values.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing metric_dict and logit_dict.\n",
    "    \"\"\"\n",
    "    metric_dict = {}\n",
    "\n",
    "    sequence_length = len(logits)\n",
    "    for idx_l in range(min(clean_generated_tokens_length, sequence_length)):\n",
    "        logit = logits[idx_l]\n",
    "        logit = logit.cpu().numpy()\n",
    "        \n",
    "        for metric, k in metrics:\n",
    "            if metric not in metric_dict:\n",
    "                metric_dict[metric] = []\n",
    "            eu = get_eu(metric, k)\n",
    "            metric_dict[metric].append(eu(logit[0]))\n",
    "\n",
    "    logit_dict = {}\n",
    "    logit_start_idx = 0\n",
    "    logit_end_idx = min(clean_generated_tokens_length, sequence_length)\n",
    "    ii = 0\n",
    "\n",
    "    # Extract topk information from logits\n",
    "    for idx_ll in range(logit_start_idx, logit_end_idx):\n",
    "        logit = logits[idx_ll]\n",
    "        logit = logit.cpu().numpy()\n",
    "        \n",
    "        top_k = 10\n",
    "        top_values, top_indices = topk(logit[0], top_k)\n",
    "        \n",
    "        logit_dict[ii] = {'top_values': top_values, 'top_indices': top_indices}\n",
    "        ii += 1\n",
    "\n",
    "    # Convert top_values and top_indices in logit_dict to list\n",
    "    for key in logit_dict:\n",
    "        logit_dict[key]['top_values'] = logit_dict[key]['top_values'].tolist()\n",
    "        logit_dict[key]['top_indices'] = logit_dict[key]['top_indices'].tolist()\n",
    "\n",
    "    return metric_dict, logit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd12628a-f5a5-443e-b5ea-955797626ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe511bb2760c4414b7ca52b3c72ef7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "if \"generator\" in globals():\n",
    "    if hasattr(generator, \"model\"):\n",
    "        del generator.model\n",
    "    if hasattr(generator, \"tokenizer\"):\n",
    "        del generator.tokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "class SemanticSpillway:\n",
    "    def __init__(self, model_path: str = \"../models/Llama-3.1-8B-Instruct\"):\n",
    "        self.device = torch.device(\"cuda\")# if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, \n",
    "            torch_dtype=torch.float16, \n",
    "            local_files_only=True\n",
    "        ).to(self.device)\n",
    "        self.reliability_scores = [] #for logtoku\n",
    "\n",
    "        #not necessary unless/until we do processing in batches(then have to do padding and attention masks and blah blah blah)\n",
    "        # if self.tokenizer.pad_token is None:\n",
    "        #     self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    # (currently) uses min p to sample next token\n",
    "    def sample_next_token(self, logits, temperature=0.7, top_k=50, top_p=0.9, min_p = 0.05, min_tokens_to_keep = 5, do_sample=True, k=25):\n",
    "        \n",
    "        # Temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # # top-k \n",
    "        # if top_k > 0:\n",
    "        #     top_k = min(top_k, logits.size(-1))\n",
    "        #     indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        #     logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # # top-p \n",
    "        # if top_p < 1.0:\n",
    "        #     sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        #     cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        #     sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        #     sorted_indices_to_remove[..., 0] = 0\n",
    "        #     indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        #     logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "        \n",
    "        # min-p truncation: gets rid of tokens w/ probabilities less than min_p * max_prob (ie probability of most likely token)\n",
    "        # pasted from paper\n",
    "        # turn into methods later i dislike this block of code\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        # Get the probability of the top token for each sequence in the batch\n",
    "        top_probs, _ = probs.max(dim=-1, keepdim=True)\n",
    "        # Calculate the actual min_p threshold by scaling min_p with the top token's probability\n",
    "        scaled_min_p = min_p * top_probs\n",
    "        # Create a mask for tokens that have a probability less than the scaled min_p\n",
    "        tokens_to_remove = probs < scaled_min_p\n",
    "\n",
    "        sorted_indices = torch.argsort(logits, descending=True, dim=-1)\n",
    "        sorted_indices_to_remove = torch.gather(tokens_to_remove, dim=-1, index=sorted_indices)\n",
    "        # Keep at least min_tokens_to_keep\n",
    "        sorted_indices_to_remove[..., : min_tokens_to_keep] = False\n",
    "\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        filter_value = float('-inf') #TODO verify that this is the correct value to use\n",
    "        scores_processed = logits.masked_fill(indices_to_remove, filter_value)\n",
    "        \n",
    "        # sample or greedy\n",
    "        if do_sample:\n",
    "            probs = torch.softmax(scores_processed, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_token = torch.argmax(scores_processed, dim=-1, keepdim=True)\n",
    "            #next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "        return next_token\n",
    "\n",
    "    # main generation loop w/ stopping control\n",
    "    # halu_detect = which function to use for hallucination control. currently only option is logtoku, but for regeneration, ner + logtoku function will be created to use instead\n",
    "    def generation_loop(self, prompt, max_new_tokens=100, temperature=1.0, top_k=50, top_p=0.9, min_p = 0.05, \n",
    "                 k=25, do_sample=True, halu_detect=None):\n",
    "\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        #turn text into token ids\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        #input_ids = input_ids.long()\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            with torch.no_grad(): #.no_grad to ignore gradients for inference\n",
    "                \n",
    "                outputs = self.model(input_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                \n",
    "                # sample next token\n",
    "                next_token = self.sample_next_token(logits, temperature,  do_sample)\n",
    "                \n",
    "                # # check for end of sequence -- if it is, end loop\n",
    "                if next_token == self.tokenizer.eos_token_id:\n",
    "                    print(\"found an eos\")\n",
    "                    break\n",
    "                #or\n",
    "                if next_token in self.tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    print(\"found an eos\")\n",
    "                    break\n",
    "                \n",
    "                # add new token to sequence\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1) \n",
    "                \n",
    "                # callback fn to implement hallucination detection for stopping generation\n",
    "                # if fn exists and returns false (aka hallucination is detected), stop generating \n",
    "                logits_1d = logits.squeeze(0).cpu().numpy()\n",
    "                if halu_detect and not halu_detect(step, self.reliability_scores, logits_1d, k):\n",
    "                    break\n",
    "                    \n",
    "        #turn token ids back into clean text\n",
    "        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "\n",
    "# returns true if no hallucination detected (clean), returns false if not\n",
    "# used as callback in greater generation loop (intended for initial generation)\n",
    "def logtoku(step, reliability_scores, logits, k=25):\n",
    "    tok_rel = get_token_reliability(logits, k)\n",
    "    #print(f\"token reliability: {tok_rel}\")\n",
    "    # if step >=20:\n",
    "    #     print(\"Hallucination detected... ceasing generation.\")\n",
    "    #     return False\n",
    "    return True\n",
    "\n",
    "# returns true if no hallucination detected (clean), returns false if not\n",
    "# used as callback in greater generation loop (intended for comparing with retrieved context)\n",
    "def ner_based():\n",
    "    #empty function to fill in later\n",
    "    return True\n",
    "\n",
    "#combining the above methods to use at once?!（⊙ｏ⊙）\n",
    "def ner_logtoku():\n",
    "    #empty\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "generator = SemanticSpillway(\"../models/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b92abe-9bd9-4c97-9f32-f76e694a970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 14.97 GB\n",
      "GPU memory cached: 15.11 GB\n",
      "GPU memory available: 23.54 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  15333 MiB |  15492 MiB |   1312 GiB |   1297 GiB |\\n|       from large pool |  15330 MiB |  15436 MiB |   1067 GiB |   1052 GiB |\\n|       from small pool |      3 MiB |     57 MiB |    244 GiB |    244 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  15333 MiB |  15492 MiB |   1312 GiB |   1297 GiB |\\n|       from large pool |  15330 MiB |  15436 MiB |   1067 GiB |   1052 GiB |\\n|       from small pool |      3 MiB |     57 MiB |    244 GiB |    244 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  15333 MiB |  15491 MiB |   1221 GiB |   1206 GiB |\\n|       from large pool |  15329 MiB |  15435 MiB |    976 GiB |    961 GiB |\\n|       from small pool |      3 MiB |     57 MiB |    244 GiB |    244 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  15472 MiB |  16914 MiB |  49094 MiB |  33622 MiB |\\n|       from large pool |  15464 MiB |  16844 MiB |  48956 MiB |  33492 MiB |\\n|       from small pool |      8 MiB |     74 MiB |    138 MiB |    130 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 141682 KiB | 177911 KiB |   1314 GiB |   1313 GiB |\\n|       from large pool | 137084 KiB | 163901 KiB |   1051 GiB |   1051 GiB |\\n|       from small pool |   4598 KiB |  31699 KiB |    262 GiB |    262 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     360    |     504    |    1229 K  |    1229 K  |\\n|       from large pool |     228    |     238    |     452 K  |     452 K  |\\n|       from small pool |     132    |     275    |     777 K  |     777 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     360    |     504    |    1229 K  |    1229 K  |\\n|       from large pool |     228    |     238    |     452 K  |     452 K  |\\n|       from small pool |     132    |     275    |     777 K  |     777 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     199    |     269    |     725    |     526    |\\n|       from large pool |     195    |     234    |     656    |     461    |\\n|       from small pool |       4    |      37    |      69    |      65    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      40    |     110    |  590224    |  590184    |\\n|       from large pool |      33    |      36    |  251385    |  251352    |\\n|       from small pool |       7    |      76    |  338839    |  338832    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check current GPU memory usage\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# See what's taking up memory\n",
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f46647ba-771a-43bc-85fa-3e0398cd6b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of France? Answer:   - \n",
      "A. Berlin\n",
      "B. Paris\n",
      "C. Madrid\n",
      "D. Rome\n",
      "Answer: B. Paris\n",
      "Explanation: Paris is the capital city of France. It is a major center of culture, fashion, and learning, and is home to the famous Louvre Museum and many other historical landmarks. Paris has been a major city for over two thousand years and is a popular destination for tourists and business travelers alike. It is located in the northern part of France, near the city of Versailles and the Seine River. Paris is known for its beautiful architecture, art, and cuisine, and is considered one of the most romantic cities in the world. The city has a population of over 2.1 million people and is the center of the Île-de-France region. The name \"Paris\" is derived from the ancient Celtic tribe known as the Parisii, who inhabited the area around 250 BC. The city has been ruled by various civilizations throughout its history, including the\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "result = generator.generation_loop(\"Question: What is the capital of France? Answer:  \", max_new_tokens=200, temperature=0.8, do_sample=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b462d9e-8174-41f8-82ce-dcd870faa274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 7\n",
      "au: 1.9106452525172894, eu: 0.06722689075630252, rel.: -0.12844673966502787\n",
      "k = 7\n",
      "au: 1.9109331796550468, eu: 0.06096897114861187, rel.: -0.1165076298973137\n",
      "k = 7\n",
      "au: 1.9072790432137179, eu: 0.06018269747447609, rel.: -0.11478519765713939\n",
      "k = 7\n",
      "au: 1.91355936389064, eu: 0.05676634566649772, rel.: -0.10862577230397957\n",
      "k = 7\n",
      "au: 1.9109155755243572, eu: 0.057318321392016376, rel.: -0.10953047311091504\n",
      "k = 7\n",
      "au: 1.9073309458369154, eu: 0.0619126589275843, rel.: -0.11808793031162772\n",
      "k = 7\n",
      "au: 1.9095557315726175, eu: 0.058885383806519455, rel.: -0.11244492215359263\n",
      "k = 7\n",
      "au: 1.9085124584650115, eu: 0.06243032329988852, rel.: -0.11914904980383573\n",
      "k = 7\n",
      "au: 1.9081736639779183, eu: 0.06130268199233716, rel.: -0.11697616330899116\n",
      "k = 7\n",
      "au: 1.919819332219023, eu: 0.05539070227497527, rel.: -0.10634014105268574\n",
      "k = 7\n",
      "au: 1.9129062809060513, eu: 0.059989287627209426, rel.: -0.11475388508916858\n",
      "k = 7\n",
      "au: 1.9100262509487758, eu: 0.06031233171782445, rel.: -0.11519813683697518\n",
      "k = 7\n",
      "au: 1.8994112498449742, eu: 0.06595995288574794, rel.: -0.1252850765504341\n",
      "k = 7\n",
      "au: 1.9058662052828867, eu: 0.06459054209919261, rel.: -0.12310093136775277\n",
      "k = 7\n",
      "au: 1.9091062253197073, eu: 0.06771463119709795, rel.: -0.12927442396360775\n",
      "k = 7\n",
      "au: 1.9102365212979762, eu: 0.059574468085106386, rel.: -0.11380132467307093\n",
      "k = 7\n",
      "au: 1.911947660774973, eu: 0.0638904734740445, rel.: -0.12215524130450485\n",
      "k = 7\n",
      "au: 1.9018816245018322, eu: 0.06470248411322935, rel.: -0.12305646559457262\n",
      "k = 7\n",
      "au: 1.885023969719963, eu: 0.06285072951739619, rel.: -0.11847513165467782\n",
      "k = 7\n",
      "au: 1.8971932107120124, eu: 0.05812143227815257, rel.: -0.11026758671496907\n",
      "k = 7\n",
      "au: 1.9152075818407401, eu: 0.059290629962943354, rel.: -0.11355386403714288\n",
      "k = 7\n",
      "au: 1.9186895190109847, eu: 0.05588822355289421, rel.: -0.10723214876708098\n",
      "k = 7\n",
      "au: 1.912772772935124, eu: 0.06222222222222222, rel.: -0.1190169725381855\n",
      "k = 7\n",
      "au: 1.914692555242471, eu: 0.06184428492545555, rel.: -0.11841279193106392\n",
      "k = 7\n",
      "au: 1.9095523626662199, eu: 0.058242329693187725, rel.: -0.11121677827281155\n",
      "k = 7\n",
      "au: 1.9098769334270853, eu: 0.06047516198704104, rel.: -0.11550011692431618\n",
      "k = 7\n",
      "au: 1.9160491542412146, eu: 0.0556936847339632, rel.: -0.10671183753108704\n",
      "k = 7\n",
      "au: 1.9057347557517765, eu: 0.06232609905397885, rel.: -0.11877701315759542\n",
      "k = 7\n",
      "au: 1.9130817433134817, eu: 0.05622489959839357, rel.: -0.10756282894132026\n",
      "k = 7\n",
      "au: 1.9126040258034664, eu: 0.05785123966942149, rel.: -0.11064651388945675\n",
      "k = 7\n",
      "au: 1.9099319125820615, eu: 0.05685279187817259, rel.: -0.10858496152750807\n",
      "k = 7\n",
      "au: 1.915637624746529, eu: 0.05400192864030858, rel.: -0.10344812631225228\n",
      "k = 7\n",
      "au: 1.9152190059437915, eu: 0.052093023255813956, rel.: -0.09976954821660682\n",
      "k = 7\n",
      "au: 1.9117156306035756, eu: 0.0563097033685269, rel.: -0.10764814008426368\n",
      "k = 7\n",
      "au: 1.9130729398495507, eu: 0.056338028169014086, rel.: -0.10777875717462258\n",
      "k = 7\n",
      "au: 1.9105458142895297, eu: 0.05673758865248227, rel.: -0.10839976251288112\n",
      "k = 7\n",
      "au: 1.9175750190375314, eu: 0.054634146341463415, rel.: -0.10476507421083098\n",
      "k = 7\n",
      "au: 1.9086179355480253, eu: 0.05705552725420275, rel.: -0.10889720263952055\n",
      "k = 7\n",
      "au: 1.912711512669714, eu: 0.05628140703517588, rel.: -0.10765009518543114\n",
      "k = 7\n",
      "au: 1.9149564538905097, eu: 0.04578904333605887, rel.: -0.08768402405385817\n",
      "k = 7\n",
      "au: 1.9052492737264635, eu: 0.050359712230215826, rel.: -0.0959478051516924\n",
      "k = 7\n",
      "au: 1.9047191482436978, eu: 0.055776892430278883, rel.: -0.10623931504148115\n",
      "k = 7\n",
      "au: 1.9133491437204966, eu: 0.051470588235294115, rel.: -0.09848120592679026\n",
      "k = 7\n",
      "au: 1.9168958119264932, eu: 0.054982817869415807, rel.: -0.10539633330180032\n",
      "k = 7\n",
      "au: 1.9058296238020485, eu: 0.06060606060606061, rel.: -0.11550482568497264\n",
      "k = 7\n",
      "au: 1.9183657826711624, eu: 0.059957173447537475, rel.: -0.11501978996743586\n",
      "k = 7\n",
      "au: 1.9020787540858222, eu: 0.05720122574055159, rel.: -0.10880123618877022\n",
      "k = 7\n",
      "au: 1.9185304789557573, eu: 0.054982817869415807, rel.: -0.10548621190134748\n",
      "k = 7\n",
      "au: 1.9178955045021644, eu: 0.053156146179401995, rel.: -0.10194793379413498\n",
      "k = 7\n",
      "au: 1.9189698644308046, eu: 0.05128205128205128, rel.: -0.09840871099645151\n",
      "k = 7\n",
      "au: 1.9110463724492495, eu: 0.0665083135391924, rel.: -0.12710047132679095\n",
      "k = 7\n",
      "au: 1.908861071092333, eu: 0.05976520811099253, rel.: -0.11408347916880539\n",
      "k = 7\n",
      "au: 1.9148513846258668, eu: 0.05904059040590406, rel.: -0.11305395628787406\n",
      "k = 7\n",
      "au: 1.9119169347350926, eu: 0.06153846153846154, rel.: -0.11765642675292878\n",
      "k = 7\n",
      "au: 1.9205297107916464, eu: 0.05114155251141553, rel.: -0.09821887105418466\n",
      "k = 7\n",
      "au: 1.9124373666596708, eu: 0.060053619302949064, rel.: -0.11484878555811429\n",
      "k = 7\n",
      "au: 1.9127539698162015, eu: 0.051423324150596875, rel.: -0.09836016741019953\n",
      "k = 7\n",
      "au: 1.9044645349032823, eu: 0.056, rel.: -0.1066500139545838\n",
      "k = 7\n",
      "au: 1.90819813757696, eu: 0.05180388529139685, rel.: -0.09885207743229395\n",
      "k = 7\n",
      "au: 1.908504848323335, eu: 0.05484818805093046, rel.: -0.10467803281695079\n",
      "k = 7\n",
      "au: 1.908895834444992, eu: 0.05550049554013875, rel.: -0.10594466474620372\n",
      "k = 7\n",
      "au: 1.8975978051928348, eu: 0.06147091108671789, rel.: -0.11664706596135978\n",
      "k = 7\n",
      "au: 1.9130943497131478, eu: 0.05525407005426739, rel.: -0.10570624921947339\n",
      "k = 7\n",
      "au: 1.9081411483175734, eu: 0.05550049554013875, rel.: -0.10590277929215472\n",
      "k = 7\n",
      "au: 1.9032697053533205, eu: 0.05608412618928393, rel.: -0.10674321832727686\n",
      "k = 7\n",
      "au: 1.9028287282551, eu: 0.05685279187817259, rel.: -0.10818112566729503\n",
      "k = 7\n",
      "au: 1.91057473242633, eu: 0.06208425720620843, rel.: -0.11861661309963913\n",
      "k = 7\n",
      "au: 1.9177633839817583, eu: 0.049910873440285206, rel.: -0.09571724554632662\n",
      "k = 7\n",
      "au: 1.9189215199527827, eu: 0.05708460754332314, rel.: -0.10954088187294173\n",
      "k = 7\n",
      "au: 1.9153280986628842, eu: 0.0619126589275843, rel.: -0.1185830553069337\n",
      "k = 7\n",
      "au: 1.918129215621226, eu: 0.05387205387205387, rel.: -0.10333356043750712\n",
      "k = 7\n",
      "au: 1.8985011204428686, eu: 0.058424621804903494, rel.: -0.11091920995806014\n",
      "k = 7\n",
      "au: 1.9071845053455219, eu: 0.04942630185348632, rel.: -0.09426507705149977\n",
      "k = 7\n",
      "au: 1.9200295909750604, eu: 0.0472972972972973, rel.: -0.09081221038395557\n",
      "k = 7\n",
      "au: 1.9136854828430767, eu: 0.0492091388400703, rel.: -0.09417081462145192\n",
      "k = 7\n",
      "au: 1.9171546290653567, eu: 0.0472972972972973, rel.: -0.0906762324557939\n",
      "k = 7\n",
      "au: 1.9016524911703523, eu: 0.04765957446808511, rel.: -0.09063194851535296\n",
      "k = 7\n",
      "au: 1.9021719233016547, eu: 0.04869565217391304, rel.: -0.09262750235208057\n",
      "k = 7\n",
      "au: 1.9139138790697832, eu: 0.05187586845761927, rel.: -0.0992859446298359\n",
      "k = 7\n",
      "au: 1.9201810009552216, eu: 0.05017921146953405, rel.: -0.09635316850671363\n",
      "k = 7\n",
      "au: 1.9035979728062271, eu: 0.05384615384615385, rel.: -0.1025014293049507\n",
      "k = 7\n",
      "au: 1.920490004538064, eu: 0.05343511450381679, rel.: -0.10262160329592708\n",
      "k = 7\n",
      "au: 1.9172771259312642, eu: 0.055090998524348254, rel.: -0.10562471131544594\n",
      "k = 7\n",
      "au: 1.919292281846937, eu: 0.04527081649151172, rel.: -0.08688792868506746\n",
      "k = 7\n",
      "au: 1.9164568518278007, eu: 0.05845511482254697, rel.: -0.11202670532605097\n",
      "k = 7\n",
      "au: 1.9235258006070532, eu: 0.04451510333863275, rel.: -0.08562594978854926\n",
      "k = 7\n",
      "au: 1.9153579642428713, eu: 0.04745762711864407, rel.: -0.09089834406576339\n",
      "k = 7\n",
      "au: 1.9138609621663978, eu: 0.050134288272157566, rel.: -0.09595005719007903\n",
      "k = 7\n",
      "au: 1.9123694068034705, eu: 0.05049594229035167, rel.: -0.0965668952037821\n",
      "k = 7\n",
      "au: 1.9128831061019864, eu: 0.051470588235294115, rel.: -0.09845721869642576\n",
      "k = 7\n",
      "au: 1.914515043290896, eu: 0.06496519721577726, rel.: -0.12437684735996539\n",
      "k = 7\n",
      "au: 1.9154621376863061, eu: 0.05860805860805861, rel.: -0.11226151722703626\n",
      "k = 7\n",
      "au: 1.9213928651311958, eu: 0.05263157894736842, rel.: -0.10112594027006293\n",
      "k = 7\n",
      "au: 1.9138604132881403, eu: 0.05333333333333334, rel.: -0.10207255537536748\n",
      "k = 7\n",
      "au: 1.895188479444814, eu: 0.05511811023622047, rel.: -0.1044592075284543\n",
      "k = 7\n",
      "au: 1.9031536037203547, eu: 0.05656565656565657, rel.: -0.10765313313973725\n",
      "k = 7\n",
      "au: 1.9127794744846573, eu: 0.059957173447537475, rel.: -0.11468485071856618\n",
      "k = 7\n",
      "au: 1.8951905999425278, eu: 0.05506391347099312, rel.: -0.10435661120627489\n",
      "k = 7\n",
      "au: 1.8847183572237927, eu: 0.059957173447537475, rel.: -0.11300238544382483\n",
      "k = 7\n",
      "au: 1.9095745163804856, eu: 0.05248359887535145, rel.: -0.10022134294030664\n",
      "k = 7\n",
      "au: 1.9196173002256862, eu: 0.045491470349309504, rel.: -0.08732621349523836\n",
      "k = 7\n",
      "au: 1.9171719769655233, eu: 0.055281342546890426, rel.: -0.10598384077993021\n",
      "k = 7\n",
      "au: 1.9096243283368157, eu: 0.053307948595906714, rel.: -0.10179815553247186\n",
      "k = 7\n",
      "au: 1.915532539474442, eu: 0.056338028169014086, rel.: -0.1079173261675742\n",
      "k = 7\n",
      "au: 1.9176620269734395, eu: 0.05351170568561873, rel.: -0.10261736599188974\n",
      "k = 7\n",
      "au: 1.9125484677862727, eu: 0.04686192468619247, rel.: -0.08962570225609312\n",
      "k = 7\n",
      "au: 1.9112682161796355, eu: 0.049777777777777775, rel.: -0.09513868453871963\n",
      "k = 7\n",
      "au: 1.9177547666920403, eu: 0.04560260586319218, rel.: -0.08745461476771518\n",
      "k = 7\n",
      "au: 1.8974994700208745, eu: 0.05363984674329502, rel.: -0.10178158076740323\n",
      "k = 7\n",
      "au: 1.9208995494737036, eu: 0.04472843450479233, rel.: -0.08591882968891965\n",
      "k = 7\n",
      "au: 1.9201176305526733, eu: 0.04590163934426229, rel.: -0.08813654697618828\n",
      "k = 7\n",
      "au: 1.893796195333701, eu: 0.052830188679245285, rel.: -0.10004961031951629\n",
      "k = 7\n",
      "au: 1.878373995877598, eu: 0.056056056056056056, rel.: -0.10529423800715264\n",
      "k = 7\n",
      "au: 1.9039502149339023, eu: 0.04690117252931323, rel.: -0.08929749751783796\n",
      "k = 7\n",
      "au: 1.91911935918213, eu: 0.04505229283990346, rel.: -0.0864607273646012\n",
      "k = 7\n",
      "au: 1.89166271263252, eu: 0.04753820033955857, rel.: -0.08992624100799755\n",
      "k = 7\n",
      "au: 1.8859369723418011, eu: 0.05137614678899083, rel.: -0.0968921747258173\n",
      "k = 7\n",
      "au: 1.9102349420659162, eu: 0.04903677758318739, rel.: -0.09367176598571918\n",
      "k = 7\n",
      "au: 1.9140231506007979, eu: 0.06726726726726727, rel.: -0.1287511068272008\n",
      "k = 7\n",
      "au: 1.9160646966039532, eu: 0.0603448275862069, rel.: -0.11562459376058339\n",
      "Answer the question concisely and accurately. Q: What were the main outcomes of the 1847 Treaty of Millbrook between the United States and the Cherokee Nation? A: The main outcomes of the Treaty of Millbrook (also known as the Treaty of New Echota) were the forced relocation of the Cherokee Nation from their ancestral lands in Georgia to Indian Territory (present-day Oklahoma) and the loss of Cherokee sovereignty. The treaty was negotiated between the U.S. government and a small faction of Cherokee leaders who were not representative of the Cherokee Nation as a whole, leading to widespread opposition and resistance from the Cherokee people. The treaty ultimately led to the Trail of Tears, a brutal and devastating forced relocation of the Cherokee Nation in 1838-1839. The treaty\n",
      "k = 7\n",
      "au: 1.9107318668195268, eu: 0.05925925925925926, rel.: -0.11322855507078677\n",
      "k = 7\n",
      "au: 1.9111776393854383, eu: 0.06565064478311841, rel.: -0.1254700443207322\n",
      "k = 7\n",
      "au: 1.913563447722569, eu: 0.06504065040650407, rel.: -0.12445941123398824\n",
      "k = 7\n",
      "au: 1.9113223473573926, eu: 0.0687116564417178, rel.: -0.13133012448099876\n",
      "k = 7\n",
      "au: 1.8921427366983206, eu: 0.05648008068582955, rel.: -0.10686837443782748\n",
      "k = 7\n",
      "au: 1.9064465395994081, eu: 0.05017921146953405, rel.: -0.09566398406592012\n",
      "k = 7\n",
      "au: 1.9036022299767672, eu: 0.05622489959839357, rel.: -0.10702984425572185\n",
      "k = 7\n",
      "au: 1.9100948742836739, eu: 0.05876180482686254, rel.: -0.11224062220344778\n",
      "k = 7\n",
      "au: 1.9103720311706773, eu: 0.06718656268746251, rel.: -0.1283513302286238\n",
      "k = 7\n",
      "au: 1.9163511230519066, eu: 0.06299212598425197, rel.: -0.12071503137334845\n",
      "k = 7\n",
      "au: 1.9170227148332168, eu: 0.05580468360737419, rel.: -0.10697884606941718\n",
      "k = 7\n",
      "au: 1.9194155329746445, eu: 0.053005205868433505, rel.: -0.10173901547239005\n",
      "k = 7\n",
      "au: 1.9188206348502093, eu: 0.05574912891986063, rel.: -0.10697257894635313\n",
      "k = 7\n",
      "au: 1.9193465124115732, eu: 0.04311008468052348, rel.: -0.08274319068133032\n",
      "k = 7\n",
      "au: 1.9175659919340258, eu: 0.05049594229035167, rel.: -0.09682930166664151\n",
      "k = 7\n",
      "au: 1.9189864695141512, eu: 0.044233807266982623, rel.: -0.08488407764043639\n",
      "k = 7\n",
      "au: 1.9219811426430569, eu: 0.04861111111111111, rel.: -0.09342963887848194\n",
      "k = 7\n",
      "au: 1.9151089561037282, eu: 0.043377226955848176, rel.: -0.0830721158340889\n",
      "k = 7\n",
      "au: 1.9201277347885628, eu: 0.0454176804541768, rel.: -0.08720774788982928\n",
      "k = 7\n",
      "au: 1.901614746424391, eu: 0.04798628963153385, rel.: -0.09125143598951663\n",
      "k = 7\n",
      "au: 1.905716658956841, eu: 0.04590163934426229, rel.: -0.08747551877178943\n",
      "k = 7\n",
      "au: 1.892567573886601, eu: 0.04794520547945205, rel.: -0.09073954121374114\n",
      "k = 7\n",
      "au: 1.9074276730373727, eu: 0.05008944543828265, rel.: -0.09554199435607592\n",
      "k = 7\n",
      "au: 1.9229816746521655, eu: 0.04284621270084162, rel.: -0.0823924818519673\n",
      "k = 7\n",
      "au: 1.916059080602197, eu: 0.05636638147961751, rel.: -0.10800131707470864\n",
      "k = 7\n",
      "au: 1.9192569724882573, eu: 0.05258215962441314, rel.: -0.10091867648764545\n",
      "k = 7\n",
      "au: 1.897426129165408, eu: 0.05484818805093046, rel.: -0.10407038514521337\n",
      "k = 7\n",
      "au: 1.9207870449426359, eu: 0.049955396966993755, rel.: -0.09595367931916825\n",
      "k = 7\n",
      "au: 1.9201486254654454, eu: 0.045639771801140996, rel.: -0.08763514509051748\n",
      "k = 7\n",
      "au: 1.9167987697493647, eu: 0.05067873303167421, rel.: -0.09714093312756962\n",
      "k = 7\n",
      "au: 1.9161677372318686, eu: 0.04844290657439446, rel.: -0.09282473467559225\n",
      "k = 7\n",
      "au: 1.92192030815766, eu: 0.04081632653061224, rel.: -0.07844572686357795\n",
      "k = 7\n",
      "au: 1.9172542817325264, eu: 0.059322033898305086, rel.: -0.1137354234926075\n",
      "k = 7\n",
      "au: 1.9208444071040605, eu: 0.04151223128243143, rel.: -0.07973853728526863\n",
      "k = 7\n",
      "au: 1.9229674742397345, eu: 0.0392156862745098, rel.: -0.07541048918587194\n",
      "k = 7\n",
      "au: 1.9190856267091283, eu: 0.05008944543828265, rel.: -0.09612593479043935\n",
      "k = 7\n",
      "au: 1.9130872700876824, eu: 0.043243243243243246, rel.: -0.08272809816595383\n",
      "k = 7\n",
      "au: 1.922869789165476, eu: 0.0426179604261796, rel.: -0.08194878857935058\n",
      "k = 7\n",
      "au: 1.9178706368999408, eu: 0.03977272727272727, rel.: -0.0762789457857931\n",
      "k = 7\n",
      "au: 1.9083402667124578, eu: 0.04697986577181208, rel.: -0.08965356957709533\n",
      "k = 7\n",
      "au: 1.9220257966142482, eu: 0.042879019908116385, rel.: -0.08241458239693561\n",
      "k = 7\n",
      "au: 1.9255059831751091, eu: 0.041791044776119404, rel.: -0.0804689067595568\n",
      "k = 7\n",
      "au: 1.9077979686108373, eu: 0.04265041888804265, rel.: -0.08136838251500905\n",
      "k = 7\n",
      "au: 1.8991501510121478, eu: 0.049557522123893805, rel.: -0.09411717562538077\n",
      "k = 7\n",
      "au: 1.9174672687605285, eu: 0.060442525634106854, rel.: -0.11589656454461911\n",
      "k = 7\n",
      "au: 1.9201952013649273, eu: 0.046166529266281946, rel.: -0.08864874796078807\n",
      "k = 7\n",
      "au: 1.9196343028363345, eu: 0.04725738396624472, rel.: -0.09071689532391117\n",
      "k = 7\n",
      "au: 1.9234321131171217, eu: 0.04717775905644482, rel.: -0.09074321679406809\n",
      "k = 7\n",
      "au: 1.908583705443173, eu: 0.045307443365695796, rel.: -0.08647304814305638\n",
      "k = 7\n",
      "au: 1.9167941751622877, eu: 0.05938494167550371, rel.: -0.1138287102959577\n",
      "k = 7\n",
      "au: 1.9187358621528703, eu: 0.058577405857740586, rel.: -0.11239456933113048\n",
      "k = 7\n",
      "au: 1.8992836361378906, eu: 0.048068669527897, rel.: -0.09129603744525483\n",
      "k = 7\n",
      "au: 1.9224731036165266, eu: 0.044059795436664044, rel.: -0.0847037716778328\n",
      "k = 7\n",
      "au: 1.9167000899586641, eu: 0.048993875765529306, rel.: -0.09390656608721364\n",
      "k = 7\n",
      "au: 1.9168429827269577, eu: 0.047217537942664416, rel.: -0.09050860626704016\n",
      "k = 7\n",
      "au: 1.9197785822420248, eu: 0.055281342546890426, rel.: -0.10612793741910502\n",
      "k = 7\n",
      "au: 1.9168505354108238, eu: 0.049601417183348095, rel.: -0.09507850308503643\n",
      "k = 7\n",
      "au: 1.922644968809971, eu: 0.04304381245196003, rel.: -0.08275796944916093\n",
      "k = 7\n",
      "au: 1.8917210870226522, eu: 0.04951370468611848, rel.: -0.09366611925134263\n",
      "k = 7\n",
      "au: 1.925894772101536, eu: 0.04426877470355731, rel.: -0.08525700176892176\n",
      "k = 7\n",
      "au: 1.9207160009480613, eu: 0.04690117252931323, rel.: -0.09008383254027758\n",
      "k = 7\n",
      "au: 1.9233167338431134, eu: 0.04713804713804714, rel.: -0.09066139486129154\n",
      "k = 7\n",
      "au: 1.8926832897420467, eu: 0.0446927374301676, rel.: -0.08458919730690712\n",
      "k = 7\n",
      "au: 1.9032039674911523, eu: 0.04903677758318739, rel.: -0.09332698964930344\n",
      "k = 7\n",
      "au: 1.915581870543258, eu: 0.06492753623188406, rel.: -0.12437401130483762\n",
      "k = 7\n",
      "au: 1.9172356173518736, eu: 0.05636638147961751, rel.: -0.1080676341939657\n",
      "k = 7\n",
      "au: 1.890278576686522, eu: 0.05444822557122023, rel.: -0.10292231433587286\n",
      "k = 7\n",
      "au: 1.9244962275225501, eu: 0.04344453064391001, rel.: -0.08360883533069265\n",
      "k = 7\n",
      "au: 1.920389136374691, eu: 0.04311008468052348, rel.: -0.08278813828867028\n",
      "k = 7\n",
      "au: 1.9214760753117361, eu: 0.04412923561859732, rel.: -0.08479327046292925\n",
      "k = 7\n",
      "au: 1.9250329593864168, eu: 0.045639771801140996, rel.: -0.08785806497607118\n",
      "k = 7\n",
      "au: 1.9211578123724813, eu: 0.05058717253839205, rel.: -0.09718594172796653\n",
      "k = 7\n",
      "au: 1.922902606037257, eu: 0.04395604395604396, rel.: -0.08452319147416515\n",
      "k = 7\n",
      "au: 1.9199282009112748, eu: 0.05442176870748299, rel.: -0.10448588848496733\n",
      "k = 7\n",
      "au: 1.9178238940078514, eu: 0.048484848484848485, rel.: -0.0929854009215928\n",
      "k = 7\n",
      "au: 1.9213303640349726, eu: 0.04597701149425287, rel.: -0.08833702823149299\n",
      "k = 7\n",
      "au: 1.919108261746012, eu: 0.05250820440693858, rel.: -0.1007689288868042\n",
      "k = 7\n",
      "au: 1.9207767915120049, eu: 0.04399057344854674, rel.: -0.0844960725252728\n",
      "k = 7\n",
      "au: 1.9209253012135368, eu: 0.0498220640569395, rel.: -0.09570446340565664\n",
      "k = 7\n",
      "au: 1.921506094081134, eu: 0.040697674418604654, rel.: -0.07820082941027871\n",
      "k = 7\n",
      "au: 1.91765705895278, eu: 0.04361370716510903, rel.: -0.08363613341227077\n",
      "k = 7\n",
      "au: 1.9249805428417792, eu: 0.04169769173492182, rel.: -0.08026724527113897\n",
      "k = 7\n",
      "au: 1.9126143405439449, eu: 0.04697986577181208, rel.: -0.0898543649919974\n",
      "k = 7\n",
      "au: 1.9123820781135616, eu: 0.038173142467621, rel.: -0.07300163352035409\n",
      "k = 7\n",
      "au: 1.9169724068326324, eu: 0.04354587869362364, rel.: -0.08347624788695755\n",
      "k = 7\n",
      "au: 1.9164890828695813, eu: 0.04052098408104197, rel.: -0.07765802361844902\n",
      "k = 7\n",
      "au: 1.9134466884463825, eu: 0.03899721448467967, rel.: -0.07461909091434361\n",
      "k = 7\n",
      "au: 1.907265346018418, eu: 0.04151223128243143, rel.: -0.07917484016088318\n",
      "k = 7\n",
      "au: 1.917239057691444, eu: 0.049601417183348095, rel.: -0.0950977743407625\n",
      "k = 7\n",
      "au: 1.9083126506118169, eu: 0.04697986577181208, rel.: -0.08965227217639408\n",
      "k = 7\n",
      "au: 1.8886382176937686, eu: 0.043076923076923075, rel.: -0.08135672322373157\n",
      "k = 7\n",
      "au: 1.9006727948322162, eu: 0.04765957446808511, rel.: -0.09058525660476946\n",
      "k = 7\n",
      "au: 1.9030342325855025, eu: 0.044059795436664044, rel.: -0.08384729899668618\n",
      "k = 7\n",
      "au: 1.918285433637011, eu: 0.04582651391162029, rel.: -0.08790833411102505\n",
      "k = 7\n",
      "au: 1.8964558982339328, eu: 0.047377326565143825, rel.: -0.0898490104070222\n",
      "k = 7\n",
      "au: 1.9198305426914017, eu: 0.04938271604938271, rel.: -0.09480644655266181\n",
      "k = 7\n",
      "au: 1.9043560659199343, eu: 0.047019311502938706, rel.: -0.08954151107600027\n",
      "k = 7\n",
      "au: 1.9102961850960667, eu: 0.045344129554655874, rel.: -0.08662071770476093\n",
      "k = 7\n",
      "au: 1.9187352302139264, eu: 0.050405040504050404, rel.: -0.09671392699548144\n",
      "k = 7\n",
      "au: 1.9140269963756322, eu: 0.04455051710421639, rel.: -0.08527089243996452\n",
      "k = 7\n",
      "au: 1.902626668321513, eu: 0.04741744284504657, rel.: -0.09021769130059672\n",
      "k = 7\n",
      "au: 1.8710477511862218, eu: 0.055308641975308645, rel.: -0.10348511018906512\n",
      "k = 7\n",
      "au: 1.8959821617847168, eu: 0.04582651391162029, rel.: -0.08688625291321124\n",
      "k = 7\n",
      "au: 1.9027406766120998, eu: 0.04725738396624472, rel.: -0.08991854674285028\n",
      "k = 7\n",
      "au: 1.919105459398706, eu: 0.040638606676342524, rel.: -0.07798977193492564\n",
      "k = 7\n",
      "au: 1.9129307756645921, eu: 0.04582651391162029, rel.: -0.08766294880296002\n",
      "k = 7\n",
      "au: 1.8998187810600138, eu: 0.045234248788368334, rel.: -0.08593687539528333\n",
      "k = 7\n",
      "au: 1.913324891269761, eu: 0.04327666151468315, rel.: -0.0828023136870994\n",
      "k = 7\n",
      "au: 1.9139474627446127, eu: 0.04643449419568822, rel.: -0.08887318234966692\n",
      "k = 7\n",
      "au: 1.9234876360740156, eu: 0.047377326565143825, rel.: -0.09112970187829515\n",
      "k = 7\n",
      "au: 1.8888944079448795, eu: 0.04925241864555849, rel.: -0.09303261815735554\n",
      "k = 7\n",
      "au: 1.9211527502506303, eu: 0.04519774011299435, rel.: -0.08683176272319233\n",
      "k = 7\n",
      "au: 1.9183113377812087, eu: 0.03943661971830986, rel.: -0.07565171472939979\n",
      "k = 7\n",
      "au: 1.908523570708788, eu: 0.05128205128205128, rel.: -0.09787300362609169\n",
      "k = 7\n",
      "au: 1.9167743575961538, eu: 0.055944055944055944, rel.: -0.10723213189349112\n",
      "k = 7\n",
      "au: 1.9252732821333898, eu: 0.043209876543209874, rel.: -0.08319082083292424\n",
      "k = 7\n",
      "au: 1.8860867067777516, eu: 0.057465366854797334, rel.: -0.10838466452494007\n",
      "k = 7\n",
      "au: 1.895756547078376, eu: 0.05026929982046679, rel.: -0.09529835425169574\n",
      "k = 7\n",
      "au: 1.9005470506045428, eu: 0.046550290939318374, rel.: -0.08847101814950491\n",
      "k = 7\n",
      "au: 1.9157018618319128, eu: 0.04357976653696498, rel.: -0.0834858398930639\n",
      "Answer the question concisely and accurately. Q: What is the capital of France? A: Paris\n",
      "The final answer is: Paris\n",
      "This response is concise and accurate, providing a direct answer to the question. It does not include any additional information or explanations that may not be relevant to the question being asked. This format is suitable for a multiple-choice question or a question that requires a direct and brief answer. \n",
      "However, if the question were to be answered in a paragraph or essay format, the response might look like this:\n",
      "\n",
      "The capital of France is Paris. Located in the north-central part of the country, Paris has been the seat of government since the 12th century and is\n"
     ]
    }
   ],
   "source": [
    "# test generation with question to trigger hallucination + logtoku detection\n",
    "result = generator.generation_loop(\n",
    "    \"Answer the question concisely and accurately. Q: What were the main outcomes of the 1847 Treaty of Millbrook between the United States and the Cherokee Nation? A:\", #this doesn't exist btw lol\n",
    "    max_new_tokens=120,\n",
    "    halu_detect=logtoku,\n",
    "    k=7,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(result)\n",
    "\n",
    "result = generator.generation_loop(\n",
    "    \"Answer the question concisely and accurately. Q: What is the capital of France? A:\",\n",
    "    max_new_tokens=120,\n",
    "    halu_detect=logtoku,\n",
    "    k=7,\n",
    "    temperature = 0.7\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08d3a5-f68d-4bc0-8f24-e3fc0cf060a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc35b1e-0433-4187-a7cd-3a5df23b62ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
