{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961e3ed-11b9-4542-9df4-1dc90dfc4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#holay molay :0 ヾ(＠⌒ー⌒＠)ノ  ╚(•⌂•)╝   (≧∀≦)ゞ\n",
    "#finish:\n",
    "#min p add ✓\n",
    "#fix sample nexttoken ✓\n",
    "#logtoku detection\n",
    "###adapt get_eu for per-token basis\n",
    "###implement R_token = -AU_token * EU_token ✓\n",
    "###logtoku fucntion(create sliding window evaluating r_response = (1/K) Σ R(aₜ) )\n",
    "#verify generation loop works\n",
    "#verify hallucination detection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41d7fc-fd97-4751-b292-03a644f1cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax, digamma\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77ab3f-d66c-4bd0-8da6-68278e6d899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for logtoku pasted from their codebase\n",
    "def topk(arr, k):\n",
    "    indices = np.argpartition(arr, -k)[-k:]\n",
    "    values = arr[indices]\n",
    "    return values, indices\n",
    "    \n",
    "def get_uncertainty_metric(logits, mode=\"prob\", k=None):\n",
    "    if mode == \"eu\":\n",
    "        if k is None:\n",
    "            raise ValueError(\"k must be provided for 'eu' mode.\")\n",
    "\n",
    "        def eu(logits):\n",
    "            top_k = k\n",
    "            if len(logits) < top_k:\n",
    "                raise ValueError(\"Logits array length is less than top_k.\")\n",
    "            top_values, _ = topk(logits, top_k)\n",
    "            mean_scores = top_k / (np.sum(np.maximum(0, top_values)) + top_k)\n",
    "            return mean_scores\n",
    "        return eu\n",
    "\n",
    "    \n",
    "    elif mode == \"au\":\n",
    "        def cal_au(logits):\n",
    "            top_k = k\n",
    "            if len(logits) < top_k:\n",
    "                raise ValueError(\"Logits array length is less than top_k.\")\n",
    "            top_values = np.partition(logits, -top_k)[-top_k:]\n",
    "            alpha = np.array([top_values])\n",
    "            alpha_0 = alpha.sum(axis=1, keepdims=True)\n",
    "            psi_alpha_k_plus_1 = digamma(alpha + 1)\n",
    "            psi_alpha_0_plus_1 = digamma(alpha_0 + 1)\n",
    "            result = - (alpha / alpha_0) * (psi_alpha_k_plus_1 - psi_alpha_0_plus_1)\n",
    "            return result.sum(axis=1)[0]\n",
    "        return cal_au\n",
    "\n",
    "        \n",
    "    elif mode == \"eu_token\":\n",
    "        def eu_token(logits):\n",
    "            score = 0\n",
    "            return score\n",
    "        return eu_token\n",
    "\n",
    "    \n",
    "    elif mode == \"au_token\":\n",
    "        def cal_au_token(logits):\n",
    "            score = 0\n",
    "            return score\n",
    "        return cal_au_token\n",
    "             \n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "def get_token_reliability(logits):\n",
    "    au = get_uncertainty_metric(logits, mode= \"au_token\")\n",
    "    eu = get_uncertainty_metric(logits, mode= \"au_token\")                            \n",
    "    return -au * eu\n",
    "\n",
    "\n",
    "#unused so far?\n",
    "def get_one_pass_metric(logits, clean_generated_tokens_length, metrics, get_eu, topk):\n",
    "    \"\"\"\n",
    "    Process logits, calculate various metrics, and extract topk information from the logits.\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "        clean_generated_tokens_length (int): Length of the generated tokens.\n",
    "        metrics (list): List of metrics and their parameters.\n",
    "        get_eu (function): Function to retrieve metric values.\n",
    "        topk (function): Function to extract topk values.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing metric_dict and logit_dict.\n",
    "    \"\"\"\n",
    "    metric_dict = {}\n",
    "\n",
    "    sequence_length = len(logits)\n",
    "    for idx_l in range(min(clean_generated_tokens_length, sequence_length)):\n",
    "        logit = logits[idx_l]\n",
    "        logit = logit.cpu().numpy()\n",
    "        \n",
    "        for metric, k in metrics:\n",
    "            if metric not in metric_dict:\n",
    "                metric_dict[metric] = []\n",
    "            eu = get_eu(metric, k)\n",
    "            metric_dict[metric].append(eu(logit[0]))\n",
    "\n",
    "    logit_dict = {}\n",
    "    logit_start_idx = 0\n",
    "    logit_end_idx = min(clean_generated_tokens_length, sequence_length)\n",
    "    ii = 0\n",
    "\n",
    "    # Extract topk information from logits\n",
    "    for idx_ll in range(logit_start_idx, logit_end_idx):\n",
    "        logit = logits[idx_ll]\n",
    "        logit = logit.cpu().numpy()\n",
    "        \n",
    "        top_k = 10\n",
    "        top_values, top_indices = topk(logit[0], top_k)\n",
    "        \n",
    "        logit_dict[ii] = {'top_values': top_values, 'top_indices': top_indices}\n",
    "        ii += 1\n",
    "\n",
    "    # Convert top_values and top_indices in logit_dict to list\n",
    "    for key in logit_dict:\n",
    "        logit_dict[key]['top_values'] = logit_dict[key]['top_values'].tolist()\n",
    "        logit_dict[key]['top_indices'] = logit_dict[key]['top_indices'].tolist()\n",
    "\n",
    "    return metric_dict, logit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12628a-f5a5-443e-b5ea-955797626ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSpillway:\n",
    "    def __init__(self, model_path: str = \"/workspace/models/Llama-3.1-8B-Instruct\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available())\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.float16, device_map=\"auto\", local_files_only=True\n",
    "        )\n",
    "        self.reliability_scores = [] #for logtoku\n",
    "\n",
    "        #not necessary unless/until we do processing in batches(then have to do padding and attention masks and blah blah blah)\n",
    "        # if self.tokenizer.pad_token is None:\n",
    "        #     self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    # (currently) uses min p to sample next token\n",
    "    def sample_next_token(self, logits, temperature=0.7, top_k=50, top_p=0.9, min_p = 0.05, do_sample=True):\n",
    "        \n",
    "        # Temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # # top-k \n",
    "        # if top_k > 0:\n",
    "        #     top_k = min(top_k, logits.size(-1))\n",
    "        #     indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        #     logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # # top-p \n",
    "        # if top_p < 1.0:\n",
    "        #     sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        #     cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        #     sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        #     sorted_indices_to_remove[..., 0] = 0\n",
    "        #     indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        #     logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "        \n",
    "        # min-p truncation: gets rid of tokens w/ probabilities less than min_p * max_prob (ie probability of most likely token)\n",
    "        # pasted directly from paper\n",
    "        # turn into methods later i dislike this block of code\n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.softmax(scores, dim=-1)\n",
    "            # Get the probability of the top token for each sequence in the batch\n",
    "            top_probs, _ = probs.max(dim=-1, keepdim=True)\n",
    "            # Calculate the actual min_p threshold by scaling min_p with the top token's probability\n",
    "            scaled_min_p = self.min_p * top_probs\n",
    "            # Create a mask for tokens that have a probability less than the scaled min_p\n",
    "            tokens_to_remove = probs < scaled_min_p\n",
    "    \n",
    "            sorted_indices = torch.argsort(scores, descending=True, dim=-1)\n",
    "            sorted_indices_to_remove = torch.gather(tokens_to_remove, dim=-1, index=sorted_indices)\n",
    "            # Keep at least min_tokens_to_keep\n",
    "            sorted_indices_to_remove[..., : self.min_tokens_to_keep] = False\n",
    "    \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            scores_processed = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "            return scores_processed\n",
    "        \n",
    "        # sample or greedy\n",
    "        if do_sample:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            return torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "    # main generation loop w/ stopping control\n",
    "    # halu_detect = which function to use for hallucination control. currently only option is logtoku, but for regeneration, ner + logtoku function will be created to use instead\n",
    "    def generation_loop(self, prompt, max_new_tokens=100, temperature=1.0, top_k=50, top_p=0.9, min_p = 0.05, \n",
    "                 do_sample=True, halu_detect=None):\n",
    "\n",
    "        #turn text into token ids\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            with torch.no_grad(): #.no_grad to ignore gradients for inference\n",
    "                outputs = self.model(input_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                \n",
    "                # sample next token\n",
    "                next_token = self.sample_next_token(logits, temperature,  do_sample)\n",
    "                \n",
    "                # check for end of sequence -- if it is, end loop\n",
    "                if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # add new token to sequence\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1) \n",
    "                \n",
    "                # callback fn to implement hallucination detection for stopping generation\n",
    "                # if fn exists and returns false (aka hallucination is detected), stop generating \n",
    "                if halu_detect and not halu_detect(step, next_token.item(), input_ids):\n",
    "                    break\n",
    "                    \n",
    "        #turn token ids back into clean text\n",
    "        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    # returns true if no hallucination detected (clean), returns false if not\n",
    "    # used as callback in greater generation loop (intended for initial generation)\n",
    "    def logtoku(step, token_id, input_ids, k=5):\n",
    "        #empty\n",
    "        return True\n",
    "    \n",
    "    # returns true if no hallucination detected (clean), returns false if not\n",
    "    # used as callback in greater generation loop (intended for comparing with retrieved context)\n",
    "    def ner_based():\n",
    "        #empty function to fill in later\n",
    "        return True\n",
    "    \n",
    "    #combining the above methods to use at once?!（⊙ｏ⊙）\n",
    "    def ner_logtoku():\n",
    "        #empty\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "# remember to edit model path!!!!!!!!!!!!!!!!! or you will be confused why the code doesnt work and actually its because you didnt make sure it was the right model path!!\n",
    "generator = SemanticSpillway(\"/workspace/models/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46647ba-771a-43bc-85fa-3e0398cd6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation\n",
    "result = generator.generation_loop(\"Once upon a time, Suzy \", max_new_tokens=50, temperature=1.2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b462d9e-8174-41f8-82ce-dcd870faa274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation with question to trigger hallucination + logtoku detection\n",
    "result = generator.generation_loop(\n",
    "    \"What were the main outcomes of the 1847 Treaty of Millbrook between the United States and the Cherokee Nation?\", #this doesn't exist btw lol\n",
    "    max_new_tokens=120,\n",
    "    halu_detect=logtoku\n",
    ")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
