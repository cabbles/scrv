{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4961e3ed-11b9-4542-9df4-1dc90dfc4bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.24.1)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m334.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "Successfully installed scipy-1.15.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.33.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#holay molay :0 ヾ(＠⌒ー⌒＠)ノ  ╚(•⌂•)╝   (≧∀≦)ゞ\n",
    "#finish:\n",
    "#min p add ✓\n",
    "#fix sample nexttoken ✓\n",
    "#logtoku detection\n",
    "###adapt get_eu for per-token basis ✓\n",
    "###implement R_token = -AU_token * EU_token ✓\n",
    "###logtoku fucntion(create sliding window evaluating r_response = (1/K) Σ R(aₜ) )\n",
    "#verify generation loop works\n",
    "#verify hallucination detection works\n",
    "!pip install transformers\n",
    "!pip install scipy\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b41d7fc-fd97-4751-b292-03a644f1cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import softmax, digamma\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c77ab3f-d66c-4bd0-8da6-68278e6d899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for logtoku pasted from their codebase\n",
    "def topk(arr, k):\n",
    "    indices = np.argpartition(arr, -k)[-k:]\n",
    "    values = arr[indices]\n",
    "    return values, indices\n",
    "    \n",
    "def get_uncertainty_metric(logits, mode=\"eu\", k=5):\n",
    "    if mode == \"eu\":\n",
    "        if k is None:\n",
    "            raise ValueError(\"k must be provided for 'eu' mode.\")\n",
    "\n",
    "        def eu(logits):\n",
    "            top_k = k\n",
    "            if len(logits) < top_k:\n",
    "                raise ValueError(\"Logits array length is less than top_k.\")\n",
    "            top_values, _ = topk(logits, top_k)\n",
    "            mean_scores = top_k / (np.sum(np.maximum(0, top_values)) + top_k)\n",
    "            return mean_scores\n",
    "        return eu\n",
    "\n",
    "    \n",
    "    elif mode == \"au\":\n",
    "        def cal_au(logits):\n",
    "            top_k = k\n",
    "            if len(logits) < top_k:\n",
    "                raise ValueError(\"Logits array length is less than top_k.\")\n",
    "            top_values = np.partition(logits, -top_k)[-top_k:]\n",
    "            alpha = np.array([top_values])\n",
    "            alpha_0 = alpha.sum(axis=1, keepdims=True)\n",
    "            psi_alpha_k_plus_1 = digamma(alpha + 1)\n",
    "            psi_alpha_0_plus_1 = digamma(alpha_0 + 1)\n",
    "            result = - (alpha / alpha_0) * (psi_alpha_k_plus_1 - psi_alpha_0_plus_1)\n",
    "            return result.sum(axis=1)[0]\n",
    "        return cal_au\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "def get_token_reliability(logits, k=25):\n",
    "    au = get_uncertainty_metric(logits, mode= \"au\")\n",
    "    eu = get_uncertainty_metric(logits, mode= \"eu\", k=25)                            \n",
    "    return -au * eu\n",
    "\n",
    "\n",
    "#unused so far?\n",
    "def get_one_pass_metric(logits, clean_generated_tokens_length, metrics, get_eu, topk):\n",
    "    \"\"\"\n",
    "    Process logits, calculate various metrics, and extract topk information from the logits.\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "        clean_generated_tokens_length (int): Length of the generated tokens.\n",
    "        metrics (list): List of metrics and their parameters.\n",
    "        get_eu (function): Function to retrieve metric values.\n",
    "        topk (function): Function to extract topk values.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing metric_dict and logit_dict.\n",
    "    \"\"\"\n",
    "    metric_dict = {}\n",
    "\n",
    "    sequence_length = len(logits)\n",
    "    for idx_l in range(min(clean_generated_tokens_length, sequence_length)):\n",
    "        logit = logits[idx_l]\n",
    "        logit = logit.cpu().numpy()\n",
    "        \n",
    "        for metric, k in metrics:\n",
    "            if metric not in metric_dict:\n",
    "                metric_dict[metric] = []\n",
    "            eu = get_eu(metric, k)\n",
    "            metric_dict[metric].append(eu(logit[0]))\n",
    "\n",
    "    logit_dict = {}\n",
    "    logit_start_idx = 0\n",
    "    logit_end_idx = min(clean_generated_tokens_length, sequence_length)\n",
    "    ii = 0\n",
    "\n",
    "    # Extract topk information from logits\n",
    "    for idx_ll in range(logit_start_idx, logit_end_idx):\n",
    "        logit = logits[idx_ll]\n",
    "        logit = logit.cpu().numpy()\n",
    "        \n",
    "        top_k = 10\n",
    "        top_values, top_indices = topk(logit[0], top_k)\n",
    "        \n",
    "        logit_dict[ii] = {'top_values': top_values, 'top_indices': top_indices}\n",
    "        ii += 1\n",
    "\n",
    "    # Convert top_values and top_indices in logit_dict to list\n",
    "    for key in logit_dict:\n",
    "        logit_dict[key]['top_values'] = logit_dict[key]['top_values'].tolist()\n",
    "        logit_dict[key]['top_indices'] = logit_dict[key]['top_indices'].tolist()\n",
    "\n",
    "    return metric_dict, logit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd12628a-f5a5-443e-b5ea-955797626ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95537d7f9d1445fca218bed6f42b16f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SemanticSpillway:\n",
    "    def __init__(self, model_path: str = \"../models/Llama-3.1-8B-Instruct\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, torch_dtype=torch.float16, local_files_only=True\n",
    "        ).to(self.device)\n",
    "        self.reliability_scores = [] #for logtoku\n",
    "\n",
    "        #not necessary unless/until we do processing in batches(then have to do padding and attention masks and blah blah blah)\n",
    "        # if self.tokenizer.pad_token is None:\n",
    "        #     self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    # (currently) uses min p to sample next token\n",
    "    def sample_next_token(self, logits, temperature=0.7, top_k=50, top_p=0.9, min_p = 0.05, min_tokens_to_keep = 5, do_sample=True):\n",
    "        \n",
    "        # Temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # # top-k \n",
    "        # if top_k > 0:\n",
    "        #     top_k = min(top_k, logits.size(-1))\n",
    "        #     indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        #     logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # # top-p \n",
    "        # if top_p < 1.0:\n",
    "        #     sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        #     cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        #     sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        #     sorted_indices_to_remove[..., 0] = 0\n",
    "        #     indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        #     logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "        \n",
    "        # min-p truncation: gets rid of tokens w/ probabilities less than min_p * max_prob (ie probability of most likely token)\n",
    "        # pasted from paper\n",
    "        # turn into methods later i dislike this block of code\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        # Get the probability of the top token for each sequence in the batch\n",
    "        top_probs, _ = probs.max(dim=-1, keepdim=True)\n",
    "        # Calculate the actual min_p threshold by scaling min_p with the top token's probability\n",
    "        scaled_min_p = min_p * top_probs\n",
    "        # Create a mask for tokens that have a probability less than the scaled min_p\n",
    "        tokens_to_remove = probs < scaled_min_p\n",
    "\n",
    "        sorted_indices = torch.argsort(logits, descending=True, dim=-1)\n",
    "        sorted_indices_to_remove = torch.gather(tokens_to_remove, dim=-1, index=sorted_indices)\n",
    "        # Keep at least min_tokens_to_keep\n",
    "        sorted_indices_to_remove[..., : min_tokens_to_keep] = False\n",
    "\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        filter_value = float('-inf') #TODO verify that this is the correct value to use\n",
    "        scores_processed = logits.masked_fill(indices_to_remove, filter_value)\n",
    "        return scores_processed\n",
    "        \n",
    "        # sample or greedy\n",
    "        if do_sample:\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            #next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "        return next_token\n",
    "\n",
    "    # main generation loop w/ stopping control\n",
    "    # halu_detect = which function to use for hallucination control. currently only option is logtoku, but for regeneration, ner + logtoku function will be created to use instead\n",
    "    def generation_loop(self, prompt, max_new_tokens=100, temperature=1.0, top_k=50, top_p=0.9, min_p = 0.05, \n",
    "                 do_sample=True, halu_detect=None):\n",
    "\n",
    "        #turn text into token ids\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        input_ids = input_ids.long()\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            with torch.no_grad(): #.no_grad to ignore gradients for inference\n",
    "                \n",
    "                outputs = self.model(input_ids.long())\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                \n",
    "                # sample next token\n",
    "                next_token = self.sample_next_token(logits, temperature,  do_sample)\n",
    "                \n",
    "                # # check for end of sequence -- if it is, end loop\n",
    "                # if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                #     break\n",
    "                \n",
    "                # add new token to sequence\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1) \n",
    "                \n",
    "                # callback fn to implement hallucination detection for stopping generation\n",
    "                # if fn exists and returns false (aka hallucination is detected), stop generating \n",
    "                if halu_detect and not halu_detect(step, next_token.item(), input_ids):\n",
    "                    break\n",
    "                    \n",
    "        #turn token ids back into clean text\n",
    "        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# returns true if no hallucination detected (clean), returns false if not\n",
    "# used as callback in greater generation loop (intended for initial generation)\n",
    "def logtoku(step, logits, token_id, input_ids, k=5):\n",
    "    #empty\n",
    "    return True\n",
    "\n",
    "# returns true if no hallucination detected (clean), returns false if not\n",
    "# used as callback in greater generation loop (intended for comparing with retrieved context)\n",
    "def ner_based():\n",
    "    #empty function to fill in later\n",
    "    return True\n",
    "\n",
    "#combining the above methods to use at once?!（⊙ｏ⊙）\n",
    "def ner_logtoku():\n",
    "    #empty\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "generator = SemanticSpillway(\"../models/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b92abe-9bd9-4c97-9f32-f76e694a970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 14.96 GB\n",
      "GPU memory cached: 15.08 GB\n",
      "GPU memory available: 23.54 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  15316 MiB |  15316 MiB |  15316 MiB |      0 B   |\\n|       from large pool |  15316 MiB |  15316 MiB |  15316 MiB |      0 B   |\\n|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  15316 MiB |  15316 MiB |  15316 MiB |      0 B   |\\n|       from large pool |  15316 MiB |  15316 MiB |  15316 MiB |      0 B   |\\n|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  15316 MiB |  15316 MiB |  15316 MiB |      0 B   |\\n|       from large pool |  15316 MiB |  15316 MiB |  15316 MiB |      0 B   |\\n|       from small pool |      0 MiB |      0 MiB |      0 MiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  15446 MiB |  15446 MiB |  15446 MiB |      0 B   |\\n|       from large pool |  15444 MiB |  15444 MiB |  15444 MiB |      0 B   |\\n|       from small pool |      2 MiB |      2 MiB |      2 MiB |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 132599 KiB | 140816 KiB | 395256 KiB | 262656 KiB |\\n|       from large pool | 131072 KiB | 139264 KiB | 393216 KiB | 262144 KiB |\\n|       from small pool |   1527 KiB |   2040 KiB |   2040 KiB |    512 KiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     292    |     292    |     292    |       0    |\\n|       from large pool |     226    |     226    |     226    |       0    |\\n|       from small pool |      66    |      66    |      66    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     292    |     292    |     292    |       0    |\\n|       from large pool |     226    |     226    |     226    |       0    |\\n|       from small pool |      66    |      66    |      66    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     195    |     195    |     195    |       0    |\\n|       from large pool |     194    |     194    |     194    |       0    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      33    |      33    |      33    |       0    |\\n|       from large pool |      32    |      32    |      32    |       0    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check current GPU memory usage\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# See what's taking up memory\n",
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bfa87-de61-45c0-bdb7-b283cf45866a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46647ba-771a-43bc-85fa-3e0398cd6b3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test generation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOnce upon a time, Suzy \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[3], line 73\u001b[0m, in \u001b[0;36mSemanticSpillway.generation_loop\u001b[0;34m(self, prompt, max_new_tokens, temperature, top_k, top_p, min_p, do_sample, halu_detect)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgeneration_loop\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, min_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m, \n\u001b[1;32m     70\u001b[0m              do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, halu_detect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m#turn text into token ids\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "result = generator.generation_loop(\"Once upon a time, Suzy \", max_new_tokens=10, temperature=1.2, do_sample=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b462d9e-8174-41f8-82ce-dcd870faa274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation with question to trigger hallucination + logtoku detection\n",
    "result = generator.generation_loop(\n",
    "    \"What were the main outcomes of the 1847 Treaty of Millbrook between the United States and the Cherokee Nation?\", #this doesn't exist btw lol\n",
    "    max_new_tokens=120,\n",
    "    halu_detect=logtoku\n",
    ")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
