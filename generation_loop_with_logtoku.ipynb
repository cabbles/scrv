{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4961e3ed-11b9-4542-9df4-1dc90dfc4bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.33.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.7.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#holay molay :0 ヾ(＠⌒ー⌒＠)ノ  ╚(•⌂•)╝   (≧∀≦)ゞ\n",
    "#finish:\n",
    "#min p add ✓\n",
    "#fix sample nexttoken ✓\n",
    "#logtoku detection\n",
    "###adapt get_eu for per-token basis ✓\n",
    "###implement R_token = -AU_token * EU_token ✓\n",
    "###logtoku fucntion(create sliding window evaluating r_response = (1/K) Σ R(aₜ) )\n",
    "#verify generation loop works ✓\n",
    "#verify hallucination detection works\n",
    "!pip install transformers\n",
    "!pip install scipy\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b41d7fc-fd97-4751-b292-03a644f1cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import softmax, digamma\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c77ab3f-d66c-4bd0-8da6-68278e6d899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topk, \n",
    "#get_uncertainty_metric (renamed from get_eu), \n",
    "#get_one_pass_metric for logtoku pasted from their codebase\n",
    "\n",
    "def topk(arr, k):\n",
    "    indices = np.argpartition(arr, -k)[-k:]\n",
    "    values = arr[indices]\n",
    "    return values, indices\n",
    "    \n",
    "def get_uncertainty_metric(logits, mode=\"eu\", k=5):\n",
    "    if mode == \"eu\":\n",
    "        if k is None:\n",
    "            raise ValueError(\"k must be provided for 'eu' mode.\")\n",
    "\n",
    "        def eu(logits):\n",
    "            top_k = k\n",
    "            if len(logits) < top_k:\n",
    "                raise ValueError(\"Logits array length is less than top_k.\")\n",
    "            top_values, _ = topk(logits, top_k)\n",
    "            mean_scores = top_k / (np.sum(np.maximum(0, top_values)) + top_k)\n",
    "            return mean_scores\n",
    "        return eu\n",
    "\n",
    "    \n",
    "    elif mode == \"au\":\n",
    "        def cal_au(logits):\n",
    "            top_k = k\n",
    "            if len(logits) < top_k:\n",
    "                raise ValueError(\"Logits array length is less than top_k.\")\n",
    "            top_values = np.partition(logits, -top_k)[-top_k:]\n",
    "            alpha = np.array([top_values])\n",
    "            alpha_0 = alpha.sum(axis=1, keepdims=True)\n",
    "            psi_alpha_k_plus_1 = digamma(alpha + 1)\n",
    "            psi_alpha_0_plus_1 = digamma(alpha_0 + 1)\n",
    "            result = - (alpha / alpha_0) * (psi_alpha_k_plus_1 - psi_alpha_0_plus_1)\n",
    "            return result.sum(axis=1)[0]\n",
    "        return cal_au\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "def get_token_reliability(logits, k=25):\n",
    "    au = get_uncertainty_metric(logits, mode= \"au\")\n",
    "    eu = get_uncertainty_metric(logits, mode= \"eu\", k=25)                            \n",
    "    return -au * eu\n",
    "\n",
    "\n",
    "#unused so far?\n",
    "def get_one_pass_metric(logits, clean_generated_tokens_length, metrics, get_eu, topk):\n",
    "    \"\"\"\n",
    "    Process logits, calculate various metrics, and extract topk information from the logits.\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "        clean_generated_tokens_length (int): Length of the generated tokens.\n",
    "        metrics (list): List of metrics and their parameters.\n",
    "        get_eu (function): Function to retrieve metric values.\n",
    "        topk (function): Function to extract topk values.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing metric_dict and logit_dict.\n",
    "    \"\"\"\n",
    "    metric_dict = {}\n",
    "\n",
    "    sequence_length = len(logits)\n",
    "    for idx_l in range(min(clean_generated_tokens_length, sequence_length)):\n",
    "        logit = logits[idx_l]\n",
    "        logit = logit.cpu().numpy()\n",
    "        \n",
    "        for metric, k in metrics:\n",
    "            if metric not in metric_dict:\n",
    "                metric_dict[metric] = []\n",
    "            eu = get_eu(metric, k)\n",
    "            metric_dict[metric].append(eu(logit[0]))\n",
    "\n",
    "    logit_dict = {}\n",
    "    logit_start_idx = 0\n",
    "    logit_end_idx = min(clean_generated_tokens_length, sequence_length)\n",
    "    ii = 0\n",
    "\n",
    "    # Extract topk information from logits\n",
    "    for idx_ll in range(logit_start_idx, logit_end_idx):\n",
    "        logit = logits[idx_ll]\n",
    "        logit = logit.cpu().numpy()\n",
    "        \n",
    "        top_k = 10\n",
    "        top_values, top_indices = topk(logit[0], top_k)\n",
    "        \n",
    "        logit_dict[ii] = {'top_values': top_values, 'top_indices': top_indices}\n",
    "        ii += 1\n",
    "\n",
    "    # Convert top_values and top_indices in logit_dict to list\n",
    "    for key in logit_dict:\n",
    "        logit_dict[key]['top_values'] = logit_dict[key]['top_values'].tolist()\n",
    "        logit_dict[key]['top_indices'] = logit_dict[key]['top_indices'].tolist()\n",
    "\n",
    "    return metric_dict, logit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c1c7f5-0321-47f1-8ded-33f3527978d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9839"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd12628a-f5a5-443e-b5ea-955797626ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f74d3b66ee34c84b4ddd602e33f3900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "if \"generator\" in globals():\n",
    "    if hasattr(generator, \"model\"):\n",
    "        del generator.model\n",
    "    if hasattr(generator, \"tokenizer\"):\n",
    "        del generator.tokenizer\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"generator variable does not exist\")\n",
    "class SemanticSpillway:\n",
    "    def __init__(self, model_path: str = \"../models/Llama-3.1-8B-Instruct\"):\n",
    "        self.device = torch.device(\"cuda\")# if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, \n",
    "            torch_dtype=torch.float16, \n",
    "            local_files_only=True\n",
    "        ).to(self.device)\n",
    "        self.reliability_scores = [] #for logtoku\n",
    "\n",
    "        #not necessary unless/until we do processing in batches(then have to do padding and attention masks and blah blah blah)\n",
    "        # if self.tokenizer.pad_token is None:\n",
    "        #     self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    # (currently) uses min p to sample next token\n",
    "    def sample_next_token(self, logits, temperature=0.7, top_k=50, top_p=0.9, min_p = 0.05, min_tokens_to_keep = 5, do_sample=True):\n",
    "        \n",
    "        # Temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # # top-k \n",
    "        # if top_k > 0:\n",
    "        #     top_k = min(top_k, logits.size(-1))\n",
    "        #     indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        #     logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # # top-p \n",
    "        # if top_p < 1.0:\n",
    "        #     sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        #     cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        #     sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        #     sorted_indices_to_remove[..., 0] = 0\n",
    "        #     indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        #     logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "        \n",
    "        # min-p truncation: gets rid of tokens w/ probabilities less than min_p * max_prob (ie probability of most likely token)\n",
    "        # pasted from paper\n",
    "        # turn into methods later i dislike this block of code\n",
    "        # Convert logits to probabilities\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        # Get the probability of the top token for each sequence in the batch\n",
    "        top_probs, _ = probs.max(dim=-1, keepdim=True)\n",
    "        # Calculate the actual min_p threshold by scaling min_p with the top token's probability\n",
    "        scaled_min_p = min_p * top_probs\n",
    "        # Create a mask for tokens that have a probability less than the scaled min_p\n",
    "        tokens_to_remove = probs < scaled_min_p\n",
    "\n",
    "        sorted_indices = torch.argsort(logits, descending=True, dim=-1)\n",
    "        sorted_indices_to_remove = torch.gather(tokens_to_remove, dim=-1, index=sorted_indices)\n",
    "        # Keep at least min_tokens_to_keep\n",
    "        sorted_indices_to_remove[..., : min_tokens_to_keep] = False\n",
    "\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        filter_value = float('-inf') #TODO verify that this is the correct value to use\n",
    "        scores_processed = logits.masked_fill(indices_to_remove, filter_value)\n",
    "        \n",
    "        # sample or greedy\n",
    "        if do_sample:\n",
    "            probs = torch.softmax(scores_processed, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_token = torch.argmax(scores_processed, dim=-1, keepdim=True)\n",
    "            #next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "        return next_token\n",
    "\n",
    "    # main generation loop w/ stopping control\n",
    "    # halu_detect = which function to use for hallucination control. currently only option is logtoku, but for regeneration, ner + logtoku function will be created to use instead\n",
    "    def generation_loop(self, prompt, max_new_tokens=100, temperature=1.0, top_k=50, top_p=0.9, min_p = 0.05, \n",
    "                 do_sample=True, halu_detect=None):\n",
    "\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        #turn text into token ids\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        #input_ids = input_ids.long()\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            with torch.no_grad(): #.no_grad to ignore gradients for inference\n",
    "                \n",
    "                outputs = self.model(input_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                \n",
    "                # sample next token\n",
    "                next_token = self.sample_next_token(logits, temperature,  do_sample)\n",
    "                \n",
    "                # # check for end of sequence -- if it is, end loop\n",
    "                if next_token == self.tokenizer.eos_token_id:\n",
    "                    print(\"found an eos\")\n",
    "                    break\n",
    "                #or\n",
    "                if next_token in self.tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    print(\"found an eos\")\n",
    "                    break\n",
    "                \n",
    "                # add new token to sequence\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1) \n",
    "                \n",
    "                # callback fn to implement hallucination detection for stopping generation\n",
    "                # if fn exists and returns false (aka hallucination is detected), stop generating \n",
    "                if halu_detect and not halu_detect(step, logits, next_token, input_ids):\n",
    "                    break\n",
    "                    \n",
    "        #turn token ids back into clean text\n",
    "        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "\n",
    "# returns true if no hallucination detected (clean), returns false if not\n",
    "# used as callback in greater generation loop (intended for initial generation)\n",
    "def logtoku(step, logits, token_id, input_ids, k=25):\n",
    "    #empty\n",
    "    if step >=20:\n",
    "        print(\"Hallucination detected... ceasing generation.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# returns true if no hallucination detected (clean), returns false if not\n",
    "# used as callback in greater generation loop (intended for comparing with retrieved context)\n",
    "def ner_based():\n",
    "    #empty function to fill in later\n",
    "    return True\n",
    "\n",
    "#combining the above methods to use at once?!（⊙ｏ⊙）\n",
    "def ner_logtoku():\n",
    "    #empty\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "generator = SemanticSpillway(\"../models/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0b92abe-9bd9-4c97-9f32-f76e694a970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 14.97 GB\n",
      "GPU memory cached: 15.10 GB\n",
      "GPU memory available: 23.54 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  15324 MiB |  15404 MiB | 601285 MiB | 585961 MiB |\\n|       from large pool |  15324 MiB |  15376 MiB | 335571 MiB | 320247 MiB |\\n|       from small pool |      0 MiB |     34 MiB | 265714 MiB | 265713 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  15324 MiB |  15404 MiB | 601285 MiB | 585961 MiB |\\n|       from large pool |  15324 MiB |  15376 MiB | 335571 MiB | 320247 MiB |\\n|       from small pool |      0 MiB |     34 MiB | 265714 MiB | 265713 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  15324 MiB |  15404 MiB | 585309 MiB | 569984 MiB |\\n|       from large pool |  15324 MiB |  15376 MiB | 319664 MiB | 304339 MiB |\\n|       from small pool |      0 MiB |     34 MiB | 265645 MiB | 265644 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  15466 MiB |  15796 MiB |  62472 MiB |  47006 MiB |\\n|       from large pool |  15464 MiB |  15752 MiB |  62372 MiB |  46908 MiB |\\n|       from small pool |      2 MiB |     44 MiB |    100 MiB |     98 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 144759 KiB | 175897 KiB | 498225 MiB | 498084 MiB |\\n|       from large pool | 143232 KiB | 165553 KiB | 211266 MiB | 211126 MiB |\\n|       from small pool |   1527 KiB |  31047 KiB | 286959 MiB | 286957 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     293    |     437    |    1012 K  |    1011 K  |\\n|       from large pool |     227    |     231    |     116 K  |     115 K  |\\n|       from small pool |      66    |     209    |     895 K  |     895 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     293    |     437    |    1012 K  |    1011 K  |\\n|       from large pool |     227    |     231    |     116 K  |     115 K  |\\n|       from small pool |      66    |     209    |     895 K  |     895 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     196    |     230    |     853    |     657    |\\n|       from large pool |     195    |     208    |     803    |     608    |\\n|       from small pool |       1    |      22    |      50    |      49    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      34    |     102    |  441900    |  441866    |\\n|       from large pool |      33    |      35    |   42352    |   42319    |\\n|       from small pool |       1    |      67    |  399548    |  399547    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Check current GPU memory usage\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# See what's taking up memory\n",
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f46647ba-771a-43bc-85fa-3e0398cd6b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a one-word answer. The Capital of France is:  Paris.\n",
      "Can we break this down? There are 32 states in India. Can I name all 32 in two seconds? No! I need a map, Wikipedia, or an Indian state geography app to be able to do so. However, for this question, the answer is already given and has to be memorized. While I'm impressed you asked this one, this is a perfect example of rote learning, which is more than memorization. But there's one way to break it up for students: \"Roast it from a Map.\" What's a map? A map is a way to present a concept using a series of relationships.\n",
      "You can say, for a Geography topic,  \"It is on the Seine River\",  \"Capital of France\", \"Paris was Capital in 1906\", \"First city built in Paris\", \"The name comes from the Latin city that is occupied from the 300BC BC.\" (Why did that become a city?) -\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "result = generator.generation_loop(\"Here's a one-word answer. The Capital of France is: \", max_new_tokens=200, temperature=1.3, do_sample=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b462d9e-8174-41f8-82ce-dcd870faa274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination detected... ceasing generation.\n",
      "What were the main outcomes of the 1847 Treaty of Millbrook between the United States and the Cherokee Nation? The treaty addressed various issues and created significant changes for the Cherokee Nation.\n",
      "Key outcomes of the 1847\n"
     ]
    }
   ],
   "source": [
    "# test generation with question to trigger hallucination + logtoku detection\n",
    "result = generator.generation_loop(\n",
    "    \"What were the main outcomes of the 1847 Treaty of Millbrook between the United States and the Cherokee Nation?\", #this doesn't exist btw lol\n",
    "    max_new_tokens=120,\n",
    "    halu_detect=logtoku\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08d3a5-f68d-4bc0-8f24-e3fc0cf060a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc35b1e-0433-4187-a7cd-3a5df23b62ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
