{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Goal: Create PerformRetrieval Function w/ Wikipedia as External Database\n",
        "\n",
        "!pip install -q wikipedia\n",
        "!pip install -q wikipedia-api"
      ],
      "metadata": {
        "id": "a1EOYdcXbbQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e8bf1a-ec4c-4649-ac1c-aef75e0e2594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
        "    user_agent='semantic-spillway-research-retrieval-mechanism/1.0 (techsuryas@gmail.com)'\n",
        ")\n",
        "\n",
        "categories = [\n",
        "    \"Science\", \"Technology\", \"Medicine\", \"Politics\",\n",
        "    \"History\", \"Current events\", \"Mathematics\",\n",
        "    \"Philosophy\", \"Ethics\", \"Computer science\", \"Artificial intelligence\",\n",
        "    \"Physics\", \"Biology\", \"Law\", \"Economics\", \"Chemistry\", \"Music\"\n",
        "]\n",
        "\n",
        "# Function to collect page titles from a category\n",
        "def get_category_members(category_name, max_depth=1, max_pages=20):\n",
        "    cat = wiki_wiki.page(\"Category:\" + category_name)\n",
        "    pages = []\n",
        "\n",
        "    def add_members(category, depth):\n",
        "        if depth > max_depth:\n",
        "            return\n",
        "        for title in category.categorymembers:\n",
        "            page = category.categorymembers[title]\n",
        "            if page.ns == wikipediaapi.Namespace.MAIN and not page.title.startswith(\"Category:\"):\n",
        "                pages.append(page.title)\n",
        "            elif page.ns == wikipediaapi.Namespace.CATEGORY:\n",
        "                add_members(page, depth + 1)\n",
        "\n",
        "    add_members(cat, 0)\n",
        "    return pages[:max_pages]\n",
        "\n",
        "# Collect all unique page titles from Wikipedia\n",
        "all_topics = set()\n",
        "for category in categories:\n",
        "    print(f\"Fetching topics from: {category}\")\n",
        "    topics = get_category_members(category, max_depth=1, max_pages=25)\n",
        "    all_topics.update(topics)\n",
        "\n",
        "print(f\"Total topics collected: {len(all_topics)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq1o79OHrXdY",
        "outputId": "834298a7-2d25-4309-d121-2f11f92ee843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching topics from: Science\n",
            "Fetching topics from: Technology\n",
            "Fetching topics from: Medicine\n",
            "Fetching topics from: Politics\n",
            "Fetching topics from: History\n",
            "Fetching topics from: Current events\n",
            "Fetching topics from: Mathematics\n",
            "Fetching topics from: Philosophy\n",
            "Fetching topics from: Ethics\n",
            "Fetching topics from: Computer science\n",
            "Fetching topics from: Artificial intelligence\n",
            "Fetching topics from: Physics\n",
            "Fetching topics from: Biology\n",
            "Fetching topics from: Law\n",
            "Fetching topics from: Economics\n",
            "Fetching topics from: Chemistry\n",
            "Total topics collected: 392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "\n",
        "with open(\"common_knowledge.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for topic in all_topics:\n",
        "        try:\n",
        "            content = wikipedia.page(topic).content\n",
        "            f.write(f\"== {topic} ==\\n\")\n",
        "            f.write(content + \"\\n\\n\")\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            f.write(f\"== {topic} (disambiguation error) ==\\n\")\n",
        "            f.write(f\"Disambiguation: {e.options[:5]}\\n\\n\")\n",
        "        except Exception as e:\n",
        "            f.write(f\"== {topic} (error) ==\\n\")\n",
        "            f.write(f\"Error: {str(e)}\\n\\n\")"
      ],
      "metadata": {
        "id": "6fJCvFilsHP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu openai tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icKFVyVEwGuv",
        "outputId": "14ce56eb-928b-45fd-f4e3-7f4fc1a7c6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tiktoken\n",
        "\n",
        "# Load corpus\n",
        "with open(\"common_knowledge.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus_text = f.read()\n",
        "\n",
        "# Tokenizer for length control\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# Simple chunking by tokens (approx. 300–400 words per chunk)\n",
        "def chunk_text(text, max_tokens=300):\n",
        "    paragraphs = text.split(\"\\n\")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for para in paragraphs:\n",
        "        current_chunk += para + \"\\n\"\n",
        "        if len(tokenizer.encode(current_chunk)) >= max_tokens:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = \"\"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(corpus_text, max_tokens=300)\n",
        "print(f\"Total chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVEh3_vAwYDC",
        "outputId": "f3c0cf40-85cb-44b4-abf3-eb4e12d2778d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 2714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key = input(\"Enter your API Key Here\"))"
      ],
      "metadata": {
        "id": "O-hmdILHwfWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import pickle\n",
        "\n",
        "def count_tokens(text, model=\"text-embedding-3-small\"):\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "def chunk_text(text, max_tokens=300, model=\"text-embedding-3-small\"):\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for word in words:\n",
        "        current_chunk.append(word)\n",
        "        if count_tokens(\" \".join(current_chunk), model=model) >= max_tokens:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "LRUqoy0mw-8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks(chunks, model=\"text-embedding-3-small\"):\n",
        "    embeddings = []\n",
        "    for chunk in chunks:\n",
        "        response = client.embeddings.create(\n",
        "            input=[chunk],\n",
        "            model=model\n",
        "        )\n",
        "        vector = response.data[0].embedding\n",
        "        embeddings.append(vector)\n",
        "    return np.array(embeddings, dtype='float32')"
      ],
      "metadata": {
        "id": "X2DVd2ZzxCUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_path = \"common_knowledge.txt\"\n",
        "\n",
        "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    full_text = f.read()\n",
        "\n",
        "chunks = chunk_text(full_text)\n",
        "print(f\"Generated {len(chunks)} chunks...\")\n",
        "\n",
        "embeddings = embed_chunks(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO5stnsjyEXU",
        "outputId": "7142c1fb-33cd-4124-f683-9735251cf424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 3005 chunks...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# Save index and chunks for reuse\n",
        "faiss.write_index(index, \"knowledge_index.faiss\")\n",
        "with open(\"chunks.pkl\", \"wb\") as f:\n",
        "    pickle.dump(chunks, f)"
      ],
      "metadata": {
        "id": "xXde_zRO1Meq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_chunks(query, k=5, model=\"text-embedding-3-small\"):\n",
        "    response = client.embeddings.create(\n",
        "        input=[query],\n",
        "        model=model\n",
        "    )\n",
        "    query_vector = np.array(response.data[0].embedding, dtype='float32').reshape(1, -1)\n",
        "\n",
        "    D, I = index.search(query_vector, k)\n",
        "    return [chunks[i] for i in I[0]]"
      ],
      "metadata": {
        "id": "UeifTzQI1OF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is Kendrick Lamar?\"\n",
        "top_chunks = retrieve_relevant_chunks(query)\n",
        "\n",
        "for i, chunk in enumerate(top_chunks):\n",
        "    print(f\"\\nChunk {i+1}:\\n{chunk}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfzk-6WE1RWT",
        "outputId": "748eda30-a5f3-4de7-a141-3f974c2dd2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunk 1:\n",
            "K-pop fans are cited as participating in everything from human rights campaigns to education programs throughout the years, often in tribute or honor of the idols they love. In 2020, large subsets of the K-pop community began a movement to disrupt a rally being held by Donald Trump as part of his reelection campaign. The rally was held at Tulsa, Oklahoma's BOK Center, with a 19,000-seat capacity. The Trump campaign in 2016 reported receiving one million ticket requests for the event. Despite the capacity and requests, the Tulsa Fire Department reported that the fire marshal counted 6,200 scanned tickets of attendees. Many believe this to be the work of the K-pop fans who began requesting tickets in large quantities following a tweet by the Trump campaign inviting supporters to register for free tickets. They did this knowing they would not attend and shared the instruction for others to do so on their social media platforms. The social media posts, especially on TikTok, garnered millions of views as the idea spread and the movement found \"Alt\" or \"Elite\" TikTok, \"on the quiet side where people do pranks and a lot of activism\". As social posts gained more views and more people created their own content, many young internet users outside these two subsets began to participate. Although K-pop and young social media users take credit themselves for the largely empty venue, and media attributed the event to these subsets, it is difficult to fully prove that they were responsible. The social posts made ahead of\n",
            "\n",
            "Chunk 2:\n",
            "it be or be not conformable to an assumed standard, is a different enquiry.\" For Austin and Bentham, a society is governed by a sovereign who has de facto authority. Through the sovereign's authority come laws, which for Austin and Bentham are commands backed by sanctions for non-compliance. Along with Hume, Bentham was an early and staunch supporter of the utilitarian concept, and was an avid prison reformer, advocate for democracy, and firm atheist. Bentham's views about law and jurisprudence were popularized by his student John Austin. Austin was the first chair of law at the new University of London, from 1829. Austin's utilitarian answer to \"what is law?\" was that law is \"commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience\". H. L. A. Hart criticized Austin and Bentham's early legal positivism because the command theory failed to account for individual's compliance with the law. ==== Hans Kelsen ==== Hans Kelsen is considered one of the preeminent jurists of the 20th century and has been highly influential in Europe and Latin America, although less so in common law countries. His Pure Theory of Law describes law as \"binding norms\", while at the same time refusing to evaluate those norms. That is, \"legal science\" is to be separated from \"legal politics\". Central to the Pure Theory of Law is the notion of a 'basic norm'\n",
            "\n",
            "Chunk 3:\n",
            "Songhai Empire. Baghayogho originated from among the Juula people, who are a Mande ethnic group composed of merchants and scholars. == History == He was born in Djenné in 1523, the son of Qadi Mahmud Bagayogo. As a youth he attended the majlis of Aḥmad b. Muḥammad b. Sa’īd, where he studied the Mukhtasar, along with the Mudawwana of Sahnun and the Muwatta Imam Malik along with his brother Ahmad. When Askia Daoud asked him to become Djenne's qadi like his father, he resisted the appointment (as had his father, to Askia Ishaq I) and took shelter in the mosque with his brother for several months before escaping to Timbuktu with their teacher. Bagayogo eventually became the Sheikh and teacher of the famous scholar Ahmed Baba at the Sankore Madrasah, one of three philosophical schools in Mali during West Africa's golden age (i.e. 12th-16th centuries); the other two were Sidi Yahya Mosque and Djinguereber Mosque. By 1583 he was a prominent enough leader that he served as de facto Qadi of Timbuktu after the death of Al-Qadi Aqib ibn Mahmud ibn Umar, issuing judgments in front of the Sidi Yahya Mosque. Bagayogo, with most\n",
            "\n",
            "Chunk 4:\n",
            "scientist who ever lived\" Leonhard Euler – pioneering Swiss mathematician and physicist Pierre-Simon Laplace – French mathematician and astronomer whose work was pivotal to the development of mathematical astronomy and statistics Alexander von Humboldt – German geographer, naturalist and explorer, and the younger brother of the Prussian minister, philosopher and linguist Wilhelm von Humboldt Charles Darwin – English naturalist, he established that all species of life have descended over time from common ancestors, and proposed the scientific theory that this branching pattern of evolution resulted from a process that he called natural selection James Clerk Maxwell – Scottish physicist and mathematician Marie Curie – Polish physicist and chemist famous for her pioneering research on radioactivity Albert Einstein – German-born theoretical physicist who developed the theory of general relativity, effecting a revolution in physics Linus Pauling – American chemist, biochemist, peace activist, author, and educator. He was one of the most influential chemists in history and ranks among the most important scientists of the 20th century John Bardeen – American physicist and electrical engineer, the only person to have won the Nobel Prize in Physics twice Frederick Sanger – English biochemist and a two-time Nobel laureate in chemistry, the only person to have been so Stephen Hawking – British theoretical physicist, cosmologist, and author == Science education == Science education Scientific literacy – encompasses written, numerical, and digital literacy as they pertain to understanding science, its methodology, observations, and theories. Pseudo-scholarship\n",
            "\n",
            "Chunk 5:\n",
            "1978, and fixed in a tangible medium of expression. Under copyright law, choreography is \"the composition and arrangement of a related series of dance movements and patterns organized into a coherent whole.\" Choreography consisting of ordinary motor activities, social dances, commonplace movements or gestures, or athletic movements may lack sufficient authorship to qualify for copyright protection. A recent lawsuit was brought by professional dancer and choreographer Kyle Hanagami, who sued Epic Games, alleging that the video game developer copied a portion of Hanagami’s copyrighted dance moves in the popular game Fortnite. Hanagami published a YouTube video in 2017 featuring a dance he choreographed to the song \"How Long\" by Charlie Puth, and Hanagami claimed that Fortnite's \"It's Complicated\" \"emote\" copied a portion of his \"How High\" choreography. Hanagami asserted claims for direct and contributory copyright infringement and unfair competition. Fortnite-maker Epic Games ultimately won dismissal of the copyright claims after the district court concluded that his two-second, four-beat sequence of dance steps was not protectable under copyright law. == See also == Ballet master Contact improvisation Dance improvisation Film editing List of choreographers List of dance awards#Choreography Beauchamp–Feuillet notation Movement director Music video Stage Directors and Choreographers Society Lists and categories Category:Ballet choreographers List of choreographers == References == == Further reading == Blom, L, A. and Tarin Chaplin,\n"
          ]
        }
      ]
    }
  ]
}